{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q. Implement a program on Adversarial training, tangent distance, tangent prop and tangent classifier. [Any three to be implemented].**"
      ],
      "metadata": {
        "id": "t-xHTU9ISaTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Simple neural network architecture for testing\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Adversarial example generation (Fast Gradient Sign Method)\n",
        "def generate_adversarial_example(model, x, y, epsilon=0.1):\n",
        "    x.requires_grad = True\n",
        "    output = model(x)\n",
        "    loss = F.cross_entropy(output, y)\n",
        "    model.zero_grad()\n",
        "    loss.backward(retain_graph=True)  # Retain the graph to use it in the next backward pass\n",
        "    x_adv = x + epsilon * x.grad.sign()\n",
        "    return x_adv\n",
        "\n",
        "# Tangent Distance Metric (measure distance in tangent space)\n",
        "def tangent_distance(x1, x2, model):\n",
        "    # Assuming model is a neural network\n",
        "    # Compute the gradients of the inputs w.r.t the output\n",
        "    x1.requires_grad = True\n",
        "    x2.requires_grad = True\n",
        "    output1 = model(x1)\n",
        "    output2 = model(x2)\n",
        "    grad1 = torch.autograd.grad(outputs=output1, inputs=x1, grad_outputs=torch.ones_like(output1), create_graph=True)[0]\n",
        "    grad2 = torch.autograd.grad(outputs=output2, inputs=x2, grad_outputs=torch.ones_like(output2), create_graph=True)[0]\n",
        "    # Compute the tangent distance as the Euclidean distance of the gradients\n",
        "    dist = torch.norm(grad1 - grad2)\n",
        "    return dist\n",
        "\n",
        "# Tangent Propagation (Train using tangent space)\n",
        "def tangent_prop(model, x, y, learning_rate=0.001, epsilon=0.1):\n",
        "    # Generate adversarial examples\n",
        "    x_adv = generate_adversarial_example(model, x, y, epsilon)\n",
        "\n",
        "    # Make sure x_adv is a leaf variable\n",
        "    x_adv = x_adv.detach().requires_grad_(True)\n",
        "\n",
        "    # Propagate the adversarial example in tangent space\n",
        "    output_adv = model(x_adv)\n",
        "    loss = F.cross_entropy(output_adv, y)\n",
        "\n",
        "    # Perform gradient descent to minimize the loss\n",
        "    model.zero_grad()\n",
        "    loss.backward(retain_graph=True)  # Retain graph to avoid the error\n",
        "    for param in model.parameters():\n",
        "        param.data -= learning_rate * param.grad.data\n",
        "    return model, loss\n",
        "\n",
        "# Training loop with adversarial training, tangent distance, and tangent prop\n",
        "def train_model(model, train_loader, epochs=5):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.view(-1, 28*28), target  # Flatten MNIST images\n",
        "            data, target = Variable(data), Variable(target)\n",
        "\n",
        "            # Adversarial Training\n",
        "            x_adv = generate_adversarial_example(model, data, target)\n",
        "\n",
        "            # Tangent Propagation\n",
        "            model, loss = tangent_prop(model, data, target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward(retain_graph=True)  # Retain graph for second backward pass\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}\")\n",
        "\n",
        "# MNIST dataset loading\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Model initialization and training\n",
        "model = SimpleNN()\n",
        "train_model(model, train_loader, epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCV-aW38Rg2W",
        "outputId": "7851f71b-d636-4506-ce6d-f7e4f729a8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.5123910307884216\n",
            "Epoch [2/5], Loss: 0.3119443953037262\n",
            "Epoch [3/5], Loss: 0.2703799903392792\n",
            "Epoch [4/5], Loss: 0.16539806127548218\n",
            "Epoch [5/5], Loss: 0.46896031498908997\n"
          ]
        }
      ]
    }
  ]
}